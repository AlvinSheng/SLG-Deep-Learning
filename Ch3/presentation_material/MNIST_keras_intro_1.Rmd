---
title: "Getting Started with Neural Networks"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## installing Keras


```{r , include=TRUE, cache=TRUE,results='hide'}
# loading data
# Install the keras R package


#install.packages("keras")


# Install the core Keras library + TensorFlow


#library(keras)
#install_keras()

```



## multiple classification




```{r , include=TRUE, cache=TRUE}
# loading data

library(keras)

reuters <- dataset_reuters(num_words = 10000)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% reuters

length(train_data)

length(test_data)
```



### prepare data


prepare X:


Essentially, the input data is the sentences with several words. All the words in a sentences are converted into the indices according to a bag-of-words.Then a sentence is essentially a list of integers. However,  We can't feed lists of integers into a neural network. We have to turn the lists into
tensors (matrix in this case).


One method is padding, say, add some 0 after the list of numbers in each observation so that the length of each observation will eventually be the same.  Another method is one-hot-encoding, which is to turn our input into vectors of 0s and 1s. 


If the i-th observation is a list [2,6,9,1], then the i-th row vector of our 2-d tensor (matrix) will be [1,1,0,0,0,1,0,0,1,0,0,0,0,0,...].


prepare y:


The labels are the different categories (binomial or multiclass). They should also be encoded in a numeric form so that we can calculate the loss (say, use a cross-entropy loss). In a multiclass(binary) case, we will use the one-hot-representation. So that we can use the cross-entropy loss later. 


If the label corresponding to the i-th observation is the second category, the one-hot-representation of $y_i$ is [0,1,0,0,0,0,0,0,...]


Note that the closed form of cross entropy loss is $-\sum_{c=1}^{M}{y_{o,c} log(p_{o,c})}$, where $c$ is the category and $o$ is the observation. We can observe that if the probability associated with the correct prediction class is higher, the loss will be smaller. Usually the cross entropy loss for each observation will be summed up.


### vectorizing data (X)

```{r , include=TRUE, cache=TRUE}


vectorize_sequences <- function(sequences, dimension = 10000) {
results <- matrix(0, nrow = length(sequences), ncol = dimension)
for (i in 1:length(sequences))
results[i, sequences[[i]]] <- 1
results
}
x_train <- vectorize_sequences(train_data)
x_test <- vectorize_sequences(test_data)


```

### vectorizing labels (y)

```{r , include=TRUE, cache=TRUE}


to_one_hot <- function(labels, dimension = 46) {
results <- matrix(0, nrow = length(labels), ncol = dimension)
for (i in 1:length(labels))
results[i, labels[[i]]] <- 1
results
}
one_hot_train_labels <- to_one_hot(train_labels)
one_hot_test_labels <- to_one_hot(test_labels)

```



The following neural network contains 3 layers. 


The first two are dense layers with 64 hidden units (nodes). Each node is with activation function ReLU, $R(z)=max(0,z)$. The third layer contains 46 nodes, as there are 46 categories in the classification tasks. If it is a binary task, we can use only one node in the last layer with a sigmoid activation function. The activation function is softmax, since we will output the classification probabilities for all the 46 classes. Note that softmax is ,$f(X_i)=\frac{exp(X_i)}{\sum_{j=1}^{M}{exp(X_j)}}$ which is a generalization of sigmoid function. 




A natural question is: why do we need (non-linear) activation functions?


If we omit the activation function, the output of each node will be $output = dot(W,input)+b$. Suppose we have $m$ intermediate layers and each layer with $n$ nodes. No matter how deep our neural network is, the final output will only fall into a space: linear transformation (affine transformation) of input data on a $n$ dimensional space. That is, a deeper neural network doesn't make the hypothesis more complex. 


However, if we use a non-linear activation function on the output, say, ReLU or Sigmoid function, the hypothesis space will be much more complex if we make the neural network deeper. 



In the compiling procedure of the model, the optimizing algorithm "rmsprop" is an unpublished gradient descent method that can change the learning rate (step size) based on the previous gradient directions;


The "accuracy" metrics is $accuracy=\frac{n}{N}$, $n$ is the number of correct predictions while $N$ is the number of total preditions.


### build the network

```{r , include=TRUE, cache=TRUE}
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = c(10000)) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 46, activation = "softmax")

model %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)

```



### Validating while training 


```{r , include=TRUE, cache=TRUE}
val_indices <- 1:1000
x_val <- x_train[val_indices,]
partial_x_train <- x_train[-val_indices,]
y_val <- one_hot_train_labels[val_indices,]
partial_y_train = one_hot_train_labels[-val_indices,]

```


### training 



```{r , include=TRUE, cache=TRUE}
history <- model %>% fit(
partial_x_train,
partial_y_train,
epochs = 20,
batch_size = 512,
validation_data = list(x_val, y_val)
)

plot(history)

```


From the above plot, we can observe that the model is already overfit on the 9-th epoch. So that we can retrain the model up to the 9-th epoch and use this model to make the predictions. 



### retraining model


```{r , include=TRUE, cache=TRUE}
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = c(10000)) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 46, activation = "softmax")


model %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)


history <- model %>% fit(
partial_x_train,
partial_y_train,
epochs = 9,
batch_size = 512,
validation_data = list(x_val, y_val)
)
results <- model %>% evaluate(x_test, one_hot_test_labels)

# print the results
results
``` 


### making predictions


The test accuracy is already reported in the results. 


```{r , include=TRUE, cache=TRUE}
predictions <- model %>% predict(x_test)
dim(predictions)
``` 




## draft code below
```{r , include=TRUE, cache=TRUE}
#word_index <- dataset_reuters_word_index()

#reverse_word_index <- names(word_index)

#names(reverse_word_index) <- word_index

#decoded_newswire <- sapply(train_data[[3]], function(index) {
#word <- if (index >= 3) reverse_word_index[[as.character(index - 3)]]
#if (!is.null(word)) word else "?"
#})

#decoded_newswire
```

