Please answer questions for Chapter 4 next to your name.

**1. In the discussion of missing values on page 93 the authors say to make sure to code the missing values consistently so that the model learns what to do with them.  Do you think that imputation methods would produce a benefit in the model?  What about multiple imputation?**

**Alvin**: 

**Antonio**:

**Cameron**:

**Dale**:

**David**:

**Deepak**:

**Jimmy**: 

**Matthew**:

**Megan**:

**Michael**: 

**Peter**:

**Rebekah**:

**Saran**:

**Yan**:


**2. Why isn’t a validation set needed with a linear regression model?  Which algorithms that you’ve used require a validation set or something like cross-validation?**

**Alvin**: 

**Antonio**:

**Cameron**:

**Dale**:

**David**:

**Deepak**:

**Jimmy**: 

**Matthew**:

**Megan**:

**Michael**: 

**Peter**:

**Rebekah**:

**Saran**:

**Yan**:

**3. To “regularize” the model you can add weight penalties such as L1 norm, L2 norm, and a combination of the two.  What statistical methods do these remind you of? What additional considerations should we keep in mind when adding in these weight terms?**

**Alvin**: 

**Antonio**:

**Cameron**:

**Dale**:

**David**:

**Deepak**:

**Jimmy**: 

**Matthew**:

**Megan**:

**Michael**: 

**Peter**:

**Rebekah**:

**Saran**:

**Yan**:


**4. What do you think of the idea of training a very big model and then using the data to make it smaller?  Have you seen this strategy used anywhere else?  What might be the pros and cons?**

**Alvin**: 

**Antonio**:

**Cameron**:

**Dale**:

**David**:

**Deepak**:

**Jimmy**: 

**Matthew**:

**Megan**:

**Michael**: 

**Peter**:

**Rebekah**:

**Saran**:

**Yan**:


**5. When have you encountered/used hyperparameters or feature engineering?**

**Alvin**: 

**Antonio**:

**Cameron**:

**Dale**:

**David**:

**Deepak**:

**Jimmy**: 

**Matthew**:

**Megan**:

**Michael**: 

**Peter**:

**Rebekah**:

**Saran**:

**Yan**:
